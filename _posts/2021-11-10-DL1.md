---
layout: post
title: "DL1 : Deep Learning Algorithms ~ Analyzing Neural Networks"
date: 2021-11-10
excerpt: In this blog, i will attempt to explain why we gravitate away from the traditional machine learning methods and focus on the important type of neural network models that form the basis for most of our pre-trianed models in deep learning. 
---
Although deep neural networks have brought promising results to key areas such as computer vision, speech recognition and language understanding, it remains a challenge to understand what exactly these networks are doing. I will attempt to firstly go through the basic idea in building a neural network attempt and subsequently explain how the scalable programming framework Tensorflow eases the creation of deep learning models through numerical computation using data flow graphs. 

Generally, there are many variants of a neural network model and they each have their specific use.

1) ANN <br>
2) RNN- recurrent networks for sequence generation <br>
3) CNN- convolutional networks for image recognition/ classification
4) GAN- generative adversarial networks for image generation

The simplest is a feed forward neural network, where the idea is to stack multiple models to discover hidden patterns.The network is represented as a nested matrix as seen in the image below : 

There are 3 important components in building a neural network : 

1) Activation function - examples are sigmoid, tanh, Rectified Linear unit(relu) <br>
2) Loss Function - which is the proxy to obtaining the ideal parameters. Example is binary cross entropy for multi-class classification problem.<br>
3) optimizer - finiding the optimal weights to the problem on hand. Examples are SGD, momentum, adam <br>





Generally, in the workings of regualr neural networks, each neuron will receive some inputs (vectors) and be transformed through a series of hidden layers, with the last layer being being the output layer perform a dot product and follow with a non-linearity. In CNN, the explicit assumption here is that the input are images.

RNN, a class of deep learning method, was first designed to help predict sequences. They work on data sequences of the variable input length. It puts the knowledge gained from the previous state as an input value for the current prediction. This is because the neurons send feedback signals to each other through hidden states. This architecture enables them to keep prior inputs in memory while predicting the current input and output, unlike feed forward neural networks. While it has been used in time series model such as the management of stock price changes, it is a technique that can also be used to predict consumer behaviour from their interaction history. Consumer history can be viewed as a sequence of events. It captures and holds important important clues that can be used for predictions. For example, we may want to find out the propensity of a customer to order within the next 7 days, from the time they engaged and viewed the product.

It can also be used to generate new text from TV script

There are two types of RNNs which are useful in prediction of time sequences via memory, LSTM and Gated RNN. LSTM is Long Short Term memory and it has 3 gates : Input / Output and Forget.

Traditionally, feature engineering was applied on sequential data through machine learning models. In feature engineering, a vector of features are constructed from a given 
consumer history and this is provided as an input to a vector based machine learning model. Deciding on the exact choice of feature representation has decisive effects on a 
model's performance and it is a tedious and laborious work, so an alternative to circumvent this is to look towards recurrent neural networks as they work directly on sequences 
as inputs.

CNN learns data compression and image denoising
