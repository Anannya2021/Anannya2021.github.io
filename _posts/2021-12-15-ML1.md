---
layout: post
title: "ML1 : Gradient Descent And Its Variants ~ Linear Regression Model"
date: 2021-12-1
excerpt: Gradient Descent is a generic method that can be used to optimize any differtiable loss function and find its minimum. We will take an in depth view of the heuristics around it.
---

In this article, i will explain how gradient descent algorithm is implemented in linear regression model using loops to minimize the cost function. In GD one goes through all the points in the training set before updating the parameters. The key parameter of this algorithm is the learning rate. If the learning rate is set to a huge or high number, then it may overshoot the local minima and if its set to a small/ low number, it will take a long time to converge.   SGD always takes a noisy random path and the parameters always oscillate. The gradient of the loss function at the current step is iterated over multiple loops and is used to guide the search and determine the direction to go.

While GD algoritms perform satisfactorily well on classical ML models, they are slow and expensive as we need to sum over all the training data points in neural networks. For this reason, faster optimizers such as momentum and adam (adaptive momment estimation) are used

<img src="/images/AI-General/gradient_descent_1.jpg" class="inline"/><br>
Direction Comparison between SGD and GD
