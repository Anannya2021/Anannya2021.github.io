---
layout: post
title: "BS1 : Blockchain ~ Hype Cycle"
date: 2021-12-1
image: /images/blockchain/Hype_Cycle_for_Blockchain.png

headerImage: true
tags: [blockchain, hype] 
---

This blog pot aims at giving readers an intuitive guide on the different methods for optimizing gradient descent. For neural networks, gradient descent is the preferred algorithm for optimization. 
Convex optimization means that the objective function has a local minimum that is equal to its global minimum. Once a minimum is found through minimizing specific convex function over convex sets, we can say we have its global minimum with confidence. In this way since the minimum is only found once, it is less computational intensive and provides for a stable and exact output in an effcient manner. One deep learning library, [keras](https://keras.io/api/optimizers/) demonstrates the implementation of the GD algorithm but it is however used as a black box optimizer during training. Mathemetically, we can represent a course function as : 



