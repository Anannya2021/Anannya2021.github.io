---
layout: post
title: "AI2 : Distruptive Decision Making ~ Can I trust you, AI?"
date: 2021-08-01
image: /images/gradient_descent.jpg
headerImage: false
tags: [ethics, fairness, inclusion] 
---
Eons ago, scientists asked this question : "Can we build models that learn from data and automatically make the right decisions and predictions?" While it seems like a rhetoric question today, where we are immersed in the field of speech recognition and pattern classifications, areas of Artificial Intelligence, we have created yet another imbalance through the areas of accountability, algorithm, process and data.  

![image](https://user-images.githubusercontent.com/80447701/147856622-c4ff794d-8130-49c8-aba2-e03ffac8a2f5.png)


One of the ramifications of living in an automated society is that we see algorithms making decisons for us, whether its an Amazon recommendation or as simple as internet query via Google search engine. The challenge is to understand that when we introduce automated decision making into the broader society, there a number of things that break or change. How do we build systems that are fair or rather prevent unfairness. There are these few notions : 

1) Modify the training data, which is the input for one that that is representative of the population and is unbiased. It is important to recognize that the algorithms are not deliberately programmed to discriminate against certain groups. Instead data providers and developers should be cognizant of their own biases and instead incorporate minority perspectives.
2) Modify the algorithm by embedding some kind of a fairness regularizer into the model

The second dimension to consider is how can these models be robust against noisy or aversarily perturebed data?

#3rd dimnesion is on reliability and interpretability. Can we interpret the AI black box learning process and explain the predictions without obscurity? Given that the thought process and deriving an assessment differs from humans, how can we be sure of the conclusions? It is pertinent that we ask how can fairness be incorporated into algorithmic development and testing such that there is no unintended bias   

Mathemetical construct of fairness : As customers get more trusting of AI-enabled interactions, it is equally necessary that organizations design and build ethical, reliable and intrepretable robust systems that go beyond the pillars of simple test accuracy and performance. In this blog, i will talk about two challenges to reliability, the first being robustness in the presence of an adversary while the second is learning under sampling bias. Trust once violated, can be diffcult to rebuild.  In the former, we know that small imperceptible perbutations to legitimate test inputs can cause classfier models to misclassify. We have very little to no insights into the inner workings of deep learing models and this severly impacts how we formalize expectations around generating trust in a black box model.  

Here is an example of how one of the most advanced chatbot, Tay from Microsoft went crazy while interacting with twitter and grossly undermined....Having inclusion principles throughout the lifecycle of AI systems to advance and ensure fairness.

<img src="/images/AI-General/AI_Microsoft_Chatbot.png" class="inline"/><br>        
`Source: http://smerity.com/arDcles/2016/tayandyou.html`

Another example is how HireVue uses an AI technique like affect recognition to decide whether an applicant deserves a [job](https://www.washingtonpost.com/technology/2019/10/22/ai-hiring-face-scanning-algorithm-increasingly-decides-whether-you-deserve-job/). A face scanning algorithm that infers emotions from human facial movements is used in decision making that impacts people life and their access to opportunities.

Yet another example is in monitoring performance at [work](https://www.theverge.com/2020/2/27/21155254/automation-robots-unemployment-jobs-vs-human-google-amazon)

References :

https://hbr.org/2020/10/when-do-we-trust-ais-recommendations-more-than-peoples

Expanding The Context - Explanability

Problems in Constructing :

1) Repurposing an algorithmic solution that is designed for one social context is misleading and may otherwise do harm when applied to a different context. 
2) 





