---
layout: post
title: "AI2 : Reliability and Artificial Intelligence ~ Can I trust you, AI?"
date: 2021-08-01
image: /images/gradient_descent.jpg
headerImage: false
tags: [ethics, fairness, inclusion] 
---
As customers get more trusting of AI-enabled interactions, it is equally necessary that organizations design and build ethical, reliable and intrepretable robust systems that go beyond the pillars of simple test accuracy and performance. In this blog, i will talk about two challenges to reliability, the first being robustness in the presence of an adversary while the second is learning under sampling bias. Trust once violated, can be diffcult to rebuild.  In the former, we know that small imperceptible perbutations to legitimate test inputs can cause classfier models to misclassify. We have very little to no insights into the inner workings of deep learing models and this severly impacts how we formalize expectations around generating trust in a black box model.  

Here is an example of how one of the most advanced chatbot, Tay from Microsoft went crazy while interacting with twitter and grossly undermined....Having inclusion principles throughout the lifecycle of AI systems to advance and ensure fairness.

<img src="/images/AI-General/AI_Microsoft_Chatbot.png" class="inline"/><br>        
`Source: http://smerity.com/arDcles/2016/tayandyou.html`

Another example is how HireVue uses an AI technique like affect recognition to decide whether an applicant deserves a [job](https://www.washingtonpost.com/technology/2019/10/22/ai-hiring-face-scanning-algorithm-increasingly-decides-whether-you-deserve-job/). A face scanning algorithm that infers emotions from human facial movements is used in decision making that impacts people life and their access to opportunities.

Yet another example is in monitoring performance at [work](https://www.theverge.com/2020/2/27/21155254/automation-robots-unemployment-jobs-vs-human-google-amazon)

References :

https://hbr.org/2020/10/when-do-we-trust-ais-recommendations-more-than-peoples






