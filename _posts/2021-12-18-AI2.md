---
layout: post
title: "AI2 : Distruptive Decision Making ~ Can I trust you, AI?"
date: 2021-08-01
image: /images/gradient_descent.jpg
headerImage: false
tags: [ethics, fairness, inclusion] 
---
One of the ramifications of living in an automated society is that we see algorithms making decisons for us, whether its an Amazon recommendation or as simple as <>. The challenge is to understand that when we introduce automated decision making into the broader society, there a number of things that break or change. How do we build systems that are fair or rather prevent unfairness. There are these few notions : 

1) Modify the training data, which is the input
2) Modify the algorithm by embedding some kind of a fairness regularizer into the model

Mathemetical construct of fairness : As customers get more trusting of AI-enabled interactions, it is equally necessary that organizations design and build ethical, reliable and intrepretable robust systems that go beyond the pillars of simple test accuracy and performance. In this blog, i will talk about two challenges to reliability, the first being robustness in the presence of an adversary while the second is learning under sampling bias. Trust once violated, can be diffcult to rebuild.  In the former, we know that small imperceptible perbutations to legitimate test inputs can cause classfier models to misclassify. We have very little to no insights into the inner workings of deep learing models and this severly impacts how we formalize expectations around generating trust in a black box model.  

Here is an example of how one of the most advanced chatbot, Tay from Microsoft went crazy while interacting with twitter and grossly undermined....Having inclusion principles throughout the lifecycle of AI systems to advance and ensure fairness.

<img src="/images/AI-General/AI_Microsoft_Chatbot.png" class="inline"/><br>        
`Source: http://smerity.com/arDcles/2016/tayandyou.html`

Another example is how HireVue uses an AI technique like affect recognition to decide whether an applicant deserves a [job](https://www.washingtonpost.com/technology/2019/10/22/ai-hiring-face-scanning-algorithm-increasingly-decides-whether-you-deserve-job/). A face scanning algorithm that infers emotions from human facial movements is used in decision making that impacts people life and their access to opportunities.

Yet another example is in monitoring performance at [work](https://www.theverge.com/2020/2/27/21155254/automation-robots-unemployment-jobs-vs-human-google-amazon)

References :

https://hbr.org/2020/10/when-do-we-trust-ais-recommendations-more-than-peoples

Expanding The Context - Explanability

Problems in Constructing :

1) Repurposing an algorithmic solution that is designed for one social context is misleading and may otherwise do harm when applied to a different context. 
2) 





