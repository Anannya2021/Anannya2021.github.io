---
layout: post
title: "MLT1 : Taking Human out of the Loop ~ Can I trust you, AI"
date: 2021-12-30
excerpt: Fairness and interpretability are amongst 2 of the many key considerations in machine learning development. Models wrapped in products need to be designed to deliver outcomes that are inclusive of a broad user base.  
---

Eons ago, scientists asked this question : "Can we build models that learn from data and automatically make the right decisions and predictions?" While it seems like a rhetoric question today, where we are immersed in the field of speech recognition and pattern classification, key branches of Artificial Intelligence, we have created yet another imbalance through the areas of accountability, algorithmic fairness and interpretability. Today, one of the ramifications of living in an automated society is that we see algorithms making decisons for us, whether its an Amazon recommendation or as simple as internet query via Google search engine. The challenge here is to understand that when we introduce automated decision making into the broader society, there a number of things that break or change. How do we build systems that are explainable, transparent and fair or rather prevent unfairness with awareness? Do we as consumers know the feature attribution that influence a model's predictions?
 
![image](https://user-images.githubusercontent.com/80447701/147856622-c4ff794d-8130-49c8-aba2-e03ffac8a2f5.png)

![image](https://user-images.githubusercontent.com/80447701/154996993-9d74085f-c717-4d3d-a0b1-08efd57a7572.png)

Biasedness can enter at any stage of the pipeline. Whether its the way we collect the data and label it, for example what is a toxic comment versus one that is the ground truth. As a case in example, facial recognition systems built trained using images of people from a certain ethnicity are rendered ineffective and biased as they could not recognize and detect those from other demographics. On the other hand, humans are notoriously bad at probabilistic decision-making, that is, aggregating information to update prior probabilities while computational learning algorithms excel at doing this very well. As AI models leverage the latest technologies in probabilistic modelling, questions related to their trustworthiness arise. Can they be robust against adversarially pertubed data? Humans have excellent visual systems and pattern recognition abilites - just look at how we solve "captcha" in proving to websites that we are humans and not bots.

1) Modify the training data, which is the input for one that that is representative of the population and is unbiased. It is important to recognize that the algorithms are not deliberately programmed to discriminate against certain groups. Instead data providers and developers should be cognizant of their own biases and instead incorporate minority perspectives. Tied to the concept of biasedness is classs imbalance. This happends when the number of samples is different for the different classes in data. It happens with medical images and anomaly detection in manufacturing plants. Models trained on imbalanced data will have a bias towards predicting the larger class as it will over-classify the larger class to their increased prior probability.


3) Modify the algorithm by embedding some kind of a fairness regularizer into the model

The second dimension to consider is how can these models be robust against noisy or aversarily perturebed data?

#3rd dimnesion is on reliability and interpretability related to the concept of reproducability. Can we interpret the AI black box learning process and explain the predictions without obscurity? Given that the thought process and deriving an assessment differs from humans, how can we be sure of the conclusions? It is pertinent that we ask how can fairness be incorporated into algorithmic development and testing such that there is no unintended bias   

Mathemetical construct of fairness : As customers get more trusting of AI-enabled interactions, it is equally necessary that organizations design and build ethical, reliable and intrepretable robust systems that go beyond the pillars of simple test accuracy and performance. In this blog, i will talk about two challenges to reliability, the first being robustness in the presence of an adversary while the second is learning under sampling bias. Trust once violated, can be diffcult to rebuild.  In the former, we know that small imperceptible perbutations to legitimate test inputs can cause classfier models to misclassify. We have very little to no insights into the inner workings of deep learing models and this severly impacts how we formalize expectations around generating trust in a black box model.  

Another related concept is aroud adopting a counterfactual fairness framework where changing a single identity term should not change the core prediction or score. Taking a hypothetical example to illustrate, 

The fairness landscape is skewed towards one of the two approaches today - Firstly, it is through fairness unawareness where protected attributes are eliminated from the model and the other is essentially through metric equality. 

Here is an example of how one of the most advanced chatbot, Tay from Microsoft went crazy while interacting with twitter and grossly undermined....Having inclusion principles throughout the lifecycle of AI systems to advance and ensure fairness.

<img src="/images/AI-General/AI_Microsoft_Chatbot.png" class="inline"/><br>        
`Source: http://smerity.com/arDcles/2016/tayandyou.html`

Another example is how HireVue uses an AI technique like affect recognition to decide whether an applicant deserves a [job](https://www.washingtonpost.com/technology/2019/10/22/ai-hiring-face-scanning-algorithm-increasingly-decides-whether-you-deserve-job/). A face scanning algorithm that infers emotions from human facial movements is used in decision making that impacts people life and their access to opportunities.

Yet another example is in monitoring performance at [work](https://www.theverge.com/2020/2/27/21155254/automation-robots-unemployment-jobs-vs-human-google-amazon)

References :

https://hbr.org/2020/10/when-do-we-trust-ais-recommendations-more-than-peoples

Expanding The Context - Explanability

Problems in Constructing :

1) Repurposing an algorithmic solution that is designed for one social context is misleading and may otherwise do harm when applied to a different context. 
2) 


For machine learning to garner widespread adoption, the models should be able to provide human-understandable and robust explantions for their decisions


