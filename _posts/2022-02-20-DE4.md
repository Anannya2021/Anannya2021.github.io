---
layout: post
title: "DE4 : Open Source Tooling ~ Machine Learning Architecture"
date: 2022-02-17
excerpt: Are propreitary MLOps platforms worth the investment or should open source alternatives be the way to go? In this blog, i share my experiment and thoughts on the preference for automating the set up for a production ready suite of MLOps tools.
---

An MLOps architecture should strive to overcome the challenges around reproducability, accountability, collaborativeness and continuos development. MLFlow is apt as an experiment tracker cum model registry as it automates model updating when it gets outdated and need to be trained on new data. It is an answer to proactively identifying model decay without manual retraining. Simply put, MLFlow is designed to track models, model parameters and hyperpararmeters for reproducability. It is an open source project that one can easily install by running the following command in anaconda prompt : 
 
* `pip install mlflow` 

You should see this if its already installed (as in my case) : 

![image](https://user-images.githubusercontent.com/80447701/147367379-72392fdc-905a-490f-8496-2811cb3f931d.png)

Goal :  A quick POC by running a simple example to train a linear regressio model and then compare the models with different values for the parameters passed as arguments to the training model.

Dataset : Wine dataset from UCI [machine learning repository](https://archive.ics.uci.edu/ml/datasets/wine+quality)

I cloned the MLflow repository via git clone https://github.com/mlflow/mlflow through GitBash :

![image](https://user-images.githubusercontent.com/80447701/147367485-99a40260-121b-46ad-83ef-404190e94601.png)

Next, cd into the examples directory with my clone of MLflow.

![image](https://user-images.githubusercontent.com/80447701/147367553-e8022420-a93c-4bd1-9de3-59745e5388a6.png)

Reproducing the code from examples/sklearn_elasticnet_wine/train.py, we try out multiple values for the parameters, alpha and l1_ratio by passing them as arguments to train.py
  
![image](https://user-images.githubusercontent.com/80447701/147367695-c2e8df29-1150-4253-be72-67466b0b6acf.png)

MLflow will log the information about each experiment run and compare the models that are produced. To start MLflow, execute the command in anaconda prompt :

* `mlflow ui` 

Call up the url : http://localhost:5000. This is how the UI looked for me, with a list of experiments run and metrics used for comapring the models : 

![image](https://user-images.githubusercontent.com/80447701/147367939-d67f5645-5664-4a36-bc4e-a087d66e71a4.png)

![image](https://user-images.githubusercontent.com/80447701/147368086-9fb70aae-952f-4e95-ad82-f01caa27f142.png)

For full tutorials and examples on MLflow, please refer [here](https://mlflow.org/docs/latest/tutorials-and-examples/index.html)

Designing and implementing an open source machine learning plaform undoubtly has its own set of challenges and unknowns on the triviality or complexity that comes with it.  

