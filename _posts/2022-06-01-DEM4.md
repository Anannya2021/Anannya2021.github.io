---
layout: post
title: "DEM4 : Running Spark on Ubuntu WSL"
date: 2022-06-01
excerpt: R
---
This blog-post explains how to run Apache Spark in a stand-alone mode on Ubuntu through WSL, a Linux distro on Windows 10.

1) Installing Java is a mandatory step in Spark. Execute the following steps to download and install OPenJDK 8. Verify the java version.

sudo apt-get update
sudo apt-get install openjdk-8-jdk

2) Next define the Java home environment variable by executing the following command to open your editor

nano ~/.bashrc. Press Ctrl + x, Shift +y, Enter to save

![image](https://user-images.githubusercontent.com/80447701/171362444-f2ce052f-198f-4eef-be14-7dae3dbc61b2.png)

3) Download and install spark.

4) Verify the spark installation through the command spark-shell for scala or the shell for python is pyspark --master local[4] for python. It links the python API to spark core and initializes the spark context. Note : to use PySpark, you will ned to have python installed on the machine.
5)  You will  see the below.

![image](https://user-images.githubusercontent.com/80447701/171364288-ec4645c7-4422-4cf3-94cc-bd6f3107b132.png)

Spark shell will also create the web UI, which can be accessed through http://localhost:4041/jobs/

![image](https://user-images.githubusercontent.com/80447701/171364879-a4739725-dc6d-4915-b27b-517c7b3060bf.png)


Next steps  explore Spark SQL engine to interact and issue SQL queries on structured data stored as tables or views in a databse from a Spark application. Instead of reading from an external json file, we can use SQL to query the table and assign the returned result to a dataframe.

With a temporary view, we can issue queries against the dataset. Similar ot Dataframes and Dataset API, we can conduct common data analysis operations. Saprk does not have a seperate metastore for Spark tables. Instead it uses the Apache Hive metastore to persist all metadata about the tables



![image](https://user-images.githubusercontent.com/80447701/171369487-138c4a09-8b48-4be7-a0c0-0ba953d7600a.png)


* To DO : How to connect MySQL database through spark driver. References : https://www.projectpro.io/apache-spark-tutorial/pyspark-tutorial
* TO DO : Sppark Dataframe vs Spark SQL  - https://www.oreilly.com/library/view/learning-spark-2nd/9781492050032/ch04.html


